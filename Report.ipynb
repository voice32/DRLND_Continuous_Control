{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Continuous Control\n",
    "\n",
    "In this project the aim was to train an agent that controls a double joint arm to reach the target point in space.\n",
    "\n",
    "A reward of +0.1 provided to an agent every time the arm is in target spot. The spot is moving, therefore the agent will receive maximum reward if it follows the spot.\n",
    "\n",
    "The observation space consists of 33 variables representing position, rotation, velocity, and angular velocities of the arm.\n",
    "\n",
    "The action space represents the torque applied to each joint. An action has continuous nature, ranging from -1 to 1.\n",
    "\n",
    "The environment is considered solved when the agent gets an average score of +30 over 100 episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithm\n",
    "\n",
    "Because the action space is continuous, it is practically impossible to use Q-table or DQN for solving this task. We will use the algorithm designed for continuous action spaces: Deep Determenistic Policy Gradient (DDPG). This approach consists of two neural networks: an actor and a critic. The first one learns the optimal policy, and the second one computes the value function.\n",
    "\n",
    "The actor has three fully-connected layers, the input layer takes the environment state and the output layer produces the torque values for each of the four action values. \n",
    "\n",
    "The critic has four fully-connected layers, and outputs the value function result.\n",
    "\n",
    "Both networks use ReLU as activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We don't know what is a good neural network setup for this task, therefore we test various network sizes (number of units per layers) in order to see, which one is the best and reaches the target score faster. We test the networks with 64, 128 and 256 units per each layer. The agents are trained for 500 episodes. Below you can see the plot of the rewards received during the training process for each agent:\n",
    "\n",
    "![](training.png)\n",
    "\n",
    "* The agent with 64-unit NN wasn't able to reach the target score in 500 epochs. The training went too slow, most probably because the generalization ability of such a small network is very low. \n",
    "* The agent with 128-unit network reached the target average score on the 285th epoch. \n",
    "* The agent with 256-unit network got to the target average score even quicker - it solved the environment in 253 epochs.\n",
    "\n",
    "So in this case possibly if we use even wider network, the training might slow down due to the computations amount, but the agent will reach the target score sooner (in terms of number of epochs required)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas for improving agent performance\n",
    "* Tweaking various hyperparameters (e.g. learning rates, update intervals)\n",
    "* Testing other activation functions\n",
    "* Using prioritized experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
